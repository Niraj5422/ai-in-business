## What are Design Cards?

**Design Cards are One Placard of Information that discusses about Activities it needs to do. The interfaces the subject-area or the domain deals with. For example, Data Architecture Management may have about 2-3 activities and Data Development may have about 4 activities for our Concerned Project.**

**Such Design Cards include ML Pipelines, Training Loops, AI Approach**

### 1. Data Governance

![https://github.com/nscalo/ai-in-business/raw/main/Metrics/images/01.DGovernance.PNG](https://github.com/nscalo/ai-in-business/raw/main/Metrics/images/01.DGovernance.PNG)

DATA METRICS
------------

- **The Data Value** on **Annotation** depends on: Sensitivity and Specificity
- **The Data Management Cost** on **Annotation** depends on: Steward Representation / Coverage
- **The Achievement of Objectives** on **Annotation** depends on: Net Promoter Score
- **The # Of Decisions Made** on **Annotation** depends on: Level of Automation
- **The Data Value** on **Availability** depends on: KPIs for Productivity
- **The Data Value** on **Security** depends on: Customer Satisfaction Score
- **The Data Value** on **Provenance** depends on: Data Bias
- **The Data Management Cost** on **Provenance** depends on: Data Formats and Transformation

1. While annotating data, the mistakes and unidentified objects in images, let's say are taken into consideration with True Positive Rate and False Positive Rate. This is termed as Specificity and Sensitivity. 

2. Annotation will cover the entire dataset and is distributed among people and data models that manually arrive at the answers. This incurs a cost to the management aspect as the tools that produce that amount of productivity is managed for automation as well as the headcount for annotating tasks and job management is an additional cost incurred to businesses. 

3. In a decision pipeline, where you've got to choose between Automation and Manual Tasks and also between Accuracy and Mistakes produced, the number of decisions made depends on the Level of Automation applied to the Annotation job.

4. While annotating, the net promoter score is used to achieve the objectives of producing valid answers to the given questions. For example, if the data annotation task has about 3 questions and 1 is neutral, the decider on how the question has been perceived on a sentiment analysis problem is a valid objective. 

5. In the topic of Availability, the tools used to obtain the dataset from given format to required format for annotation is a matter of Productivity and hence KPIs come into the problem solving context. 

6. How secure the job related to applying data is dependent on the satisfaction and dissatisfaction score provided by the customer or the client. So CSS (Customer Satisfaction Score) is a key metric that determines Security.

7. Data Bias is a key factor of historical provenance and origins of the data because that is how the trained model will produce its output. 

8. The Cost on Provenance is dependent on the formats of data and data transformation pipelines that are used. 

MODEL BUILDING METRICS
----------------------

- **The Data Value** on **Feature Extraction** depends on: Classification Uncertainty and Consistency
- **The Data Value** on **Hyperparameters** depends on: Completeness
- **The Data Value** on **Tuning** depends on: Accuracy
- **The Data Value** on **Model Selection** depends on: Loss Functions and Model / Information Currency
- **The Data Value** on **Benchmarking** depends on: Validity
- **The Data Management Cost** on **Feature Extraction** depends on: Complexity and Invariancy
- **The Data Management Cost** on **Hyperparameters** depends on: Error Functions
- **The Data Management Cost** on **Tuning** depends on: Test Dataset
- **The Data Management Cost** on **Model Selection** depends on: Model size, Performance and Serialization of Model
- **The Data Management Cost** on **Benchmarking** depends on: Execution Environment
- **The # of Decisions Made** on **Feature Extraction** depends on: Dataset Size

1. The data value statistics inferred from feature extracton process is relevant to how the data labels are classified. Classification uncertainty explains why each data item is classified in that data label or class. Consistency is required for such feature extraction process in order to make the process repeatable. 

2. For example, using grid search cross validation algorithm, you can actually find the hyperparameters of a machine learning algorithm. In finding so, the dataset is aimed towards completeness which is in reality matched with predefined attributes that relate to the dataset. Those pre-defined attributes define the dataset to explain it in a particular way. 

3. In Tuning the model, Accuracy is really the key. 

4. Loss Functions and Data currency of the model or the currency of the information it handles are key factors that make the model gets selected from one or two different models. 

5. Validation dataset is the key factor that is considered for benchmarking. Benchmarking depends on Validation loss and it is taken into consideration for model selection.

6. Complexity and Invariancy are cost parameters that enable feature extraction. Cost and complexity drive value whereas invariancy of data transformation helps the model achieve its objectives. 

7. Error Functions determine the hyperparameters - Training error must be optimal and bias-variance tradeoff must be balanced. Hyperparameters are chosen in order to navigate the tradeoff between bias and variance. 

8. Additional Test Data in a test dataset helps in tuning the model for performance. 

9. When the model is serialized the performance increases and when the size of the model is optimal, the cost of the model as well reduces. 

10. The execution environment is well dependent on the benchmarking of the model. If the model is run across multi-devices they are to be modified accordingly. The deployment of the model such as to the cloud or the edge varies. 

11. With regards to the dataset size used for training, the inference makes a number of decisions to achieve the objective. Such varied dataset sizes are sourced from bias on data attributes such as labels. So decisions made in the ML or AI pipeline will change accordingly. 

